{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8304f55e-621c-4c4c-b06b-bbd999438f66",
   "metadata": {},
   "source": [
    "# Loading most if not all Libraries needed or plan to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee52eb-c94a-481e-ad7a-7afe35ad7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import panel as pn\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "from yahoofinancials import YahooFinancials\n",
    "import json\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "pn.extension(\"plotly\")\n",
    "from panel.interact import interact\n",
    "import hvplot.pandas\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8dc05-06ed-43a5-9c63-9cd7934e3686",
   "metadata": {},
   "source": [
    "# Loading the environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9aeaf7-31eb-48e2-a5c1-2607413e9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set News API Key\n",
    "newsapi = NewsApiClient(api_key=os.environ[\"NEWS_API\"])\n",
    "#api_newsdata = NewsDataApiClient(apikey=\"newsdata\")\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version='v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c95eb4-4834-44c8-a2b9-188942444dcf",
   "metadata": {},
   "source": [
    "# Defining Functions to pull news items and to calculate the sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecabbd9-d100-4d12-80d8-13d95ad63f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Functions\n",
    "# Use newsapi client to get most relevant 20 headlines per day in the past month\n",
    "def get_headlines(keyword,fdate,edate):\n",
    "    all_headlines = []\n",
    "    all_contents = []\n",
    "    all_dates = [] \n",
    "    # string conversion of dates passed in to function\n",
    "    fdate_str=str(fdate.strftime(\"%Y-%m-%d\"))\n",
    "    edate_str=str(edate.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    print(f\"Fetching news about '{keyword}'\")\n",
    "    print(\"*\" * 30)\n",
    "    #while date > end_date:\n",
    "    print(f\"retrieving news from: {fdate} - {edate}\")\n",
    "    articles = newsapi.get_everything(\n",
    "        q=keyword,\n",
    "        # from_param=str(date)[:10],\n",
    "        # to=str(date)[:10],\n",
    "        from_param=fdate_str,\n",
    "        to=edate_str,\n",
    "        language=\"en\",\n",
    "        sort_by=\"relevancy\",\n",
    "        page=1,\n",
    "    )\n",
    "    headlines = []\n",
    "    contents = []\n",
    "    for i in range(0, len(articles[\"articles\"])):\n",
    "        headlines.append(articles[\"articles\"][i][\"title\"])\n",
    "        contents.append(articles[\"articles\"][i][\"content\"])\n",
    "    all_headlines.append(headlines)\n",
    "    all_contents.append(contents)\n",
    "    all_dates.append(fdate)\n",
    "    #date = date - timedelta(weeks=1)\n",
    "    return all_headlines, all_dates, all_contents\n",
    "\n",
    "# Create function that computes average compound sentiment of headlines for each day\n",
    "def headline_sentiment_summarizer_avg(data,sdate):\n",
    "    df=data.copy()\n",
    "    sentiment = []\n",
    "    sentiment_pos = []\n",
    "    sentiment_neg = []\n",
    "    for day in data:\n",
    "        day_score = []\n",
    "        day_positive = []\n",
    "        day_negative = []\n",
    "        for h in day:\n",
    "            \n",
    "            if h == None:\n",
    "                continue\n",
    "            else:\n",
    "                day_score.append(sid.polarity_scores(h)[\"compound\"])\n",
    "                day_positive.append(sid.polarity_scores(h)[\"pos\"])\n",
    "                day_negative.append(sid.polarity_scores(h)[\"neg\"])\n",
    "        sentiment.append(sum(day_score) / len(day_score))\n",
    "        sentiment_pos.append(sum(day_positive) / len(day_positive))\n",
    "        sentiment_neg.append(sum(day_negative) / len(day_negative))\n",
    "    d={\"c0\":sentiment,\"p0\":sentiment_pos,\"n0\":sentiment_neg,\"Date\":str(sdate.strftime(\"%Y-%m-%d\"))}\n",
    "    sentiment_df=pd.DataFrame(data=d).set_index(\"Date\")\n",
    "    return sentiment_df #, sentiment_pos, sentiment_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba57dc8-7aa6-448b-a67c-5a3fcbfd59fe",
   "metadata": {},
   "source": [
    "# Pulling in Asset Values into the Dataframe from yahoo financials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7eee6-62d4-4748-9724-1c56900193c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call and set API from yahoo financials - crude oil price\n",
    "yahoo_financials = YahooFinancials('CL=F')\n",
    "crude_prices=(yahoo_financials.get_historical_price_data(\"2020-06-01\", \"2022-06-01\", \"weekly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1c83a-30b3-4497-820b-138a03f118ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set json object and write crrude prices to json\n",
    "json_object= json.dumps(crude_prices['CL=F']['prices'], indent = 4)\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db108c71-bcfb-44a2-823d-4c85af4870fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json\n",
    "crude_prices = pd.read_json('sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95221168-8954-490f-b61f-b807c63fd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for crude prices \n",
    "crude_prices_df = pd.DataFrame(crude_prices)\n",
    "#crude_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c1ecf-d486-436a-86ca-f2e3eb1d8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns \n",
    "crude_prices_df.drop(['date','high','low','open','adjclose'],axis=1, inplace = True)\n",
    "#crude_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685335da-00a9-4ea6-bd3f-95137e513f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns and set index of dataframe on date\n",
    "crude_prices_df.rename(columns = {'close':'Crude Close', 'volume':'Crude Volume','formatted_date':'Date'}, inplace = True)\n",
    "crude_prices_df.set_index('Date', inplace = True)\n",
    "#crude_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd0edc-f6ee-4f9a-ab9d-c17f74feafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call yahoo financial to get gold prices \n",
    "yahoo_financials = YahooFinancials('GC=F')\n",
    "gold_prices=(yahoo_financials.get_historical_price_data(\"2020-06-01\", \"2022-06-01\", \"weekly\"))\n",
    "#print(gold_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d0f1a-05ed-4dd8-87af-20e748a9c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set json object and write gold prices to json\n",
    "json_object= json.dumps(gold_prices['GC=F']['prices'], indent = 4)\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70f181-09e6-45e2-94d0-ae9115e11c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json\n",
    "gold_prices = pd.read_json('sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10edada-afc1-4016-b27a-f866f21f2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert gold prices to dataframe\n",
    "gold_prices_df = pd.DataFrame(gold_prices)\n",
    "#gold_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c389bb-755f-44b3-9460-faec3ad088de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns from the data frame\n",
    "gold_prices_df.drop(['date','high','low','open','adjclose'],axis=1, inplace = True)\n",
    "#gold_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ff152-54d7-45ac-bcac-76b5dbc94eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns and set index to date \n",
    "gold_prices_df.rename(columns = {'close':'Gold Close', 'volume':'Gold Volume','formatted_date':'Date'}, inplace = True)\n",
    "gold_prices_df.set_index('Date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea88fd-1e5e-4217-be3b-4843f9ee059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call api for SPY s and p 500 data\n",
    "yahoo_financials = YahooFinancials('SPY')\n",
    "SPY_prices=(yahoo_financials.get_historical_price_data(\"2020-06-01\", \"2022-06-01\", \"weekly\"))\n",
    "#print(SPY_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2ae9a-c686-4f13-a6c7-705c6f29df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set json object and write data \n",
    "json_object= json.dumps(SPY_prices['SPY']['prices'], indent = 4)\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af85efb-28da-44f6-9740-a540c998eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the json data\n",
    "SPY_prices = pd.read_json('sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9b503-1307-4d42-9aa0-2bcd117074d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert json to a data frame\n",
    "SPY_prices_df = pd.DataFrame(SPY_prices)\n",
    "#SPY_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e8e0c-e23c-4a10-b812-05861741bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneedeeed columns, rename columns and set index to date \n",
    "SPY_prices_df.drop(['date','high','low','open','adjclose'],axis=1, inplace = True)\n",
    "SPY_prices_df.rename(columns = {'close':'SPY Close', 'formatted_date':'Date','volume':'SPY Volume'}, inplace = True)\n",
    "SPY_prices_df.set_index('Date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8776b4-455b-4638-935d-e227ab04cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call api and get bitcoin prices \n",
    "yahoo_financials = YahooFinancials('BTC-USD')\n",
    "BTC_prices=(yahoo_financials.get_historical_price_data(\"2020-06-01\", \"2022-06-01\", \"weekly\"))\n",
    "#print(BTC_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcc8c7-8728-4a1e-a936-8fcf60aa0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set sson object and write data\n",
    "json_object= json.dumps(BTC_prices['BTC-USD']['prices'], indent = 4)\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f90702-e03a-496a-bef4-978ca806cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json data\n",
    "BTC_prices = pd.read_json('sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84f127-db4e-4a67-9367-ee7e0ad0480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Btc data frame\n",
    "BTC_prices_df = pd.DataFrame(BTC_prices)\n",
    "#BTC_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c98ff6-99f4-4fb1-b390-bb6de9dba6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop undeeded columns, rename columns set index of data frame to date\n",
    "BTC_prices_df.drop(['date','high','low','open','adjclose'],axis=1, inplace = True)\n",
    "BTC_prices_df.rename(columns = {'close':'BTC Close', 'volume':'BTC Volume','formatted_date':'Date'}, inplace = True)\n",
    "BTC_prices_df.set_index('Date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dc5a1-18bc-469d-9d10-1b36a76d70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call yahoo financial api for eth data\n",
    "yahoo_financials = YahooFinancials('ETH-USD')\n",
    "ETH_prices=(yahoo_financials.get_historical_price_data(\"2020-06-01\", \"2022-06-01\", \"weekly\"))\n",
    "#print(ETH_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c6947-291b-4d55-8717-00a15536f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set json obect write data to json\n",
    "json_object= json.dumps(ETH_prices['ETH-USD']['prices'], indent = 4)\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb82ce0-a0bc-4788-beba-695b5d570622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json data\n",
    "ETH_prices = pd.read_json('sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e07db9-6192-48f9-89bf-25709d71e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create eath data frame\n",
    "ETH_prices_df = pd.DataFrame(ETH_prices)\n",
    "#ETH_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ffcfe-fc6a-4935-b8b5-620365c3f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns, rename columns, set index to date\n",
    "ETH_prices_df.drop(['date','high','low','open','adjclose'],axis=1, inplace = True)\n",
    "ETH_prices_df.rename(columns = {'close':'ETH Close', 'volume':'ETH Volume','formatted_date':'Date'}, inplace = True)\n",
    "ETH_prices_df.set_index('Date', inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae09f61-a3f1-4a47-963f-82127e5b4c80",
   "metadata": {},
   "source": [
    "# Combining Asset Data into a single dataframe and setting index to DateTime, Adding feature using pandas lambda function to create a binary determination on last two weeks of data based on price for each asset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20523d34-86ed-41b2-a9cc-a8ec1b7b86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate asset dataframes into one data frame\n",
    "combined_asset_data = pd.concat([crude_prices_df, gold_prices_df, BTC_prices_df, ETH_prices_df,SPY_prices_df], axis=\"columns\", join=\"inner\")\n",
    "\n",
    "# Adding Features to Predict On to determine if the Asset should move up or down in 2 periods\n",
    "combined_asset_data[ 'SPY UP(2)' ] = combined_asset_data['SPY Close'].diff(periods=2).apply(lambda x: 1 if x <= 0 else 0)\n",
    "combined_asset_data[ 'CRUDE UP(2)' ] = combined_asset_data['Crude Close'].diff(periods=2).apply(lambda x: 1 if x <= 0 else 0)\n",
    "combined_asset_data[ 'GOLD UP(2)' ] = combined_asset_data['Gold Close'].diff(periods=2).apply(lambda x: 1 if x <= 0 else 0)\n",
    "combined_asset_data[ 'BTC UP(2)' ] = combined_asset_data['BTC Close'].diff(periods=2).apply(lambda x: 1 if x <= 0 else 0)\n",
    "combined_asset_data[ 'ETH UP(2)' ] = combined_asset_data['ETH Close'].diff(periods=2).apply(lambda x: 1 if x <= 0 else 0)\n",
    "\n",
    "# Setting Index to datetime to assist with merging with sentiments \n",
    "combined_asset_data.index = pd.to_datetime(combined_asset_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c13f1-a74f-42cf-9e04-8b10ec398c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "061d7f11-4d6d-46b8-a061-f3bd6b15982e",
   "metadata": {},
   "source": [
    "# The code below is commented out but was used to pull the news api data for 2 years. This is using a paid plan and to minimize cost we do not run unless our saved sentiment scores get corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bb73b-209a-4196-88da-a7727bc34c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid = SentimentIntensityAnalyzer()\n",
    "# sp_sentiment_df=pd.DataFrame()\n",
    "# gold_sentiment_df=pd.DataFrame()\n",
    "# crude_sentiment_df=pd.DataFrame()\n",
    "# btc_sentiment_df=pd.DataFrame()\n",
    "# eth_sentiment_df=pd.DataFrame()\n",
    "# for fdate in pd.to_datetime(pd.Series(list(combined_asset_data.index.values))):\n",
    "#     edate = fdate\n",
    "#     fdate = fdate - timedelta(days=6)\n",
    "#     #S&P 500\n",
    "#     sp_headlines, sp_dates, sp_contents = get_headlines(\"S&P 500\",fdate,edate)\n",
    "#     temp_sp_df = headline_sentiment_summarizer_avg(sp_contents,fdate)\n",
    "#     sp_sentiment_df = pd.concat([sp_sentiment_df,temp_sp_df])\n",
    "#     # Gold\n",
    "#     gold_headlines, gold_dates, gold_contents = get_headlines(\"Gold\",fdate,edate)\n",
    "#     temp_gold_df = headline_sentiment_summarizer_avg(gold_contents,fdate)\n",
    "#     gold_sentiment_df = pd.concat([gold_sentiment_df,temp_gold_df])\n",
    "#     # Crude Oil\n",
    "#     crude_headlines, crude_dates, crude_contents = get_headlines(\"Crude Oil\",fdate,edate)\n",
    "#     temp_crude_df = headline_sentiment_summarizer_avg(crude_contents,fdate)\n",
    "#     crude_sentiment_df = pd.concat([crude_sentiment_df,temp_crude_df])\n",
    "#     # Bitcoin\n",
    "#     btc_headlines, btc_dates, btc_contents = get_headlines(\"Bitcoin\",fdate,edate)\n",
    "#     temp_btc_df = headline_sentiment_summarizer_avg(btc_contents,fdate)\n",
    "#     btc_sentiment_df = pd.concat([btc_sentiment_df,temp_btc_df])\n",
    "#     # Ethereum\n",
    "#     eth_headlines, eth_dates, eth_contents = get_headlines(\"Ethereum\",fdate,edate)\n",
    "#     temp_eth_df = headline_sentiment_summarizer_avg(eth_contents,fdate)\n",
    "#     eth_sentiment_df = pd.concat([eth_sentiment_df,temp_eth_df])\n",
    "\n",
    "# #Save sentiment score dataframes to disk to use later if needed (Api calls run out)\n",
    "# sp_sentiment_df.to_csv('./sp_sentiment.csv')\n",
    "# gold_sentiment_df.to_csv('./gold_sentiment.csv')\n",
    "# crude_sentiment_df.to_csv('./crude_sentiment.csv')\n",
    "# btc_sentiment_df.to_csv('./btc_sentiment.csv')\n",
    "# eth_sentiment_df.to_csv('./eth_sentiment.csv')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5abed-8abc-4b19-a3d2-8dd3a9494d0f",
   "metadata": {},
   "source": [
    "# The commented out code below used to save sentiment scores as csv file that were compiled from news api code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e782972-7de7-4365-98af-658202ea828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save sentiment score dataframes to disk to use later if needed (Api calls run out)\n",
    "# sp_sentiment_df.to_csv('./sp_sentiment.csv')\n",
    "# gold_sentiment_df.to_csv('./gold_sentiment.csv')\n",
    "# crude_sentiment_df.to_csv('./crude_sentiment.csv')\n",
    "# btc_sentiment_df.to_csv('./btc_sentiment.csv')\n",
    "# eth_sentiment_df.to_csv('./eth_sentiment.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c261fa-834c-4d00-849c-53d10811b46d",
   "metadata": {},
   "source": [
    "# The code below loads in the saved sentiment code from the 2 commented sections above that generated and saved it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc632463-aebd-4e3a-908b-15687b12b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved sentiment scores \n",
    "sp_sentiment_df = pd.read_csv('./sp_sentiment.csv', index_col='Date', infer_datetime_format=True)\n",
    "gold_sentiment_df = pd.read_csv('./gold_sentiment.csv', index_col='Date', infer_datetime_format=True)\n",
    "crude_sentiment_df = pd.read_csv('./crude_sentiment.csv', index_col='Date', infer_datetime_format=True)\n",
    "btc_sentiment_df = pd.read_csv('./btc_sentiment.csv', index_col='Date', infer_datetime_format=True)\n",
    "eth_sentiment_df = pd.read_csv('./eth_sentiment.csv', index_col='Date', infer_datetime_format=True)\n",
    "\n",
    "# renaming sentiment columns\n",
    "sp_sentiment_df.columns=['sp500 c0','sp500 p0', 'sp500 n0']\n",
    "gold_sentiment_df.columns=['gold c0','gold p0', 'gold n0']\n",
    "crude_sentiment_df.columns=['crude c0','crude p0', 'crude n0']\n",
    "btc_sentiment_df.columns=['btc c0','btc p0', 'btc n0']\n",
    "eth_sentiment_df.columns=['eth c0','eth p0', 'eth n0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b216076-e0e5-4c62-bd82-dabf10ab3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining sentiment dataframes for each asset into a single sentiment dataframe\n",
    "all_sentiment_df = pd.concat([sp_sentiment_df,gold_sentiment_df,crude_sentiment_df,btc_sentiment_df,eth_sentiment_df], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dbb3a-c711-4ad4-98b1-708fcd2f6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f136a67-0622-4d6e-961e-29a59b86eee3",
   "metadata": {},
   "source": [
    "# Modeling Price on Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8220d1-876a-4c13-88c4-9864ec2ee7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa34132-ac59-484b-ab7c-859671e90142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed \n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b91e3-c2ff-4060-b044-279242cb1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature x and target y dataframes\n",
    "#X = combined_asset_data.iloc[:, 0:8]\n",
    "X = all_sentiment_df.copy()\n",
    "y = combined_asset_data.iloc[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a308636-6694-4450-99dc-d974f6c6f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data of the features set using the StandardScaler, imports \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "#set scalare to standard scaler\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler().fit(X)\n",
    "# X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95c2fe-e82d-41a7-b581-c5f7b1250348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use 70% of the data for training and the remaineder for testing\n",
    "split = int(0.7 * len(X))\n",
    "\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76472343-0f73-4aaf-a885-8eec564dc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fit the Scaler object with the features data X\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# # Scale the features training and testing sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Fit theScaler object with the target data Y\n",
    "scaler.fit(y_train)\n",
    "\n",
    "# # Scale the target training and testing sets\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f018a-8c95-4ac2-845a-d678cfaf35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras models and layers \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "# Create a shallow, 1 hidden layer, neural network\n",
    "nn = Sequential()\n",
    "\n",
    "# Hidden layer\n",
    "nn.add(Dense(units=2, input_dim=15, activation=\"relu\"))\n",
    "nn.add(Dense(units=2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(Dense(units=1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc6990-4df0-4e4a-8826-aac916b3b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# Fit the model set epochs\n",
    "model_1 = nn.fit(X, y, validation_split=0.3, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6b6dc-19c8-436b-91db-ab7256a3d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function of the training results for the model\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_1.history[\"loss\"])\n",
    "#plt.plot(model_2.history[\"loss\"])\n",
    "plt.title(\"loss_function - Training -2 hinden layer\")\n",
    "plt.legend([\"1 hidden layer\", \"2 hidden layers\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d63d7a-6ec5-4671-ad81-0897d7d98585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape the features for the model\n",
    "# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac077f-c692-4b61-afec-197519ab1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The return sequences need to be set to True if you are adding additional LSTM layers, but \n",
    "# You don't have to do this for the final layer. \n",
    "# Note: The dropouts help prevent overfitting\n",
    "# Note: The input shape is the number of time steps and the number of indicators\n",
    "# Note: Batching inputs has a different input shape of Samples/TimeSteps/Features\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Initial model setup\n",
    "number_units = 8\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 3\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f621b44-efee-4120-b770-cf4134f68b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d6f7f-d41d-4015-844f-ce6ba141fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4571a6-b740-4de9-ba7f-e191133cade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Use at least 10 epochs\n",
    "# Do not shuffle the data\n",
    "# Experiement with the batch size, but a smaller batch size is recommended\n",
    "model.fit(X_train, y_train, epochs=250, shuffle=False, batch_size=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862817c-4c41-4b2a-92fd-02ca09955e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8ef49-17d2-41d8-b76f-e854feb459d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64571669-af14-4536-b288-5c25d10c9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the original prices instead of the scaled version\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65eeb1-158e-4684-813d-84eb2ee4dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of Real and Predicted values\n",
    "assets = pd.DataFrame({\n",
    "    \"Real\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "}, index = combined_asset_data.index[-len(real_prices): ]) \n",
    "assets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5ac84-6ab2-4d8d-9b70-8e592847a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the real vs predicted values as a line chart\n",
    "assets.plot(title=\"Actual Vs. Predicted SPY Prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601da82-f239-4903-a9f4-010527af1d25",
   "metadata": {},
   "source": [
    "# Predicting Market direction of SP500 with neural networks using the compiled sentiment scores\n",
    "\n",
    "This code train a neural network model to predict whether a market environment for asset will be positive or negative \n",
    "\n",
    "The dataset contains 2 years of data on a weekly basis or 105 records with sentiment scores to be used \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574aea5-0b2c-436f-b4f3-c9efaa149e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features data\n",
    "X = all_sentiment_df.copy()\n",
    "\n",
    "# Define target data\n",
    "y = combined_asset_data['SPY UP(2)'].values\n",
    "y = y.reshape(-1, 1)\n",
    "y_crude = combined_asset_data['CRUDE UP(2)'].values\n",
    "y_crude = y_crude.reshape(-1, 1)\n",
    "y_gold = combined_asset_data['GOLD UP(2)'].values\n",
    "y_gold = y_gold.reshape(-1, 1)\n",
    "y_btc = combined_asset_data['BTC UP(2)'].values\n",
    "y_btc = y_btc.reshape(-1, 1)\n",
    "y_eth = combined_asset_data['ETH UP(2)'].values\n",
    "y_eth = y_eth.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e2100-e0e3-4e33-9598-fc7eef3e8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.7 * len(X))\n",
    "\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "\n",
    "#SPY Definition\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cfdc2-eb84-42f5-8d18-d009d5f59731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler instance\n",
    "# Most likely not needed as this is already a binary set defined above\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the features data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993a126-8797-4e00-9a14-b63ec9c55fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "number_inputs = 15\n",
    "number_hidden_nodes = 37\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units=number_hidden_nodes, input_dim=number_inputs, activation=\"relu\"))\n",
    "nn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3c637-8eb0-4cd8-a873-0adceae7f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = nn.fit(X_train_scaled, y_train, epochs=250, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933bf3a8-edba-40e3-87ac-6f761c92ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "nn_json = nn.to_json()\n",
    "\n",
    "file_path = Path(\"./model.json\")\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json_file.write(nn_json)\n",
    "\n",
    "# Save weights\n",
    "file_path = \"./model.h5\"\n",
    "nn.save_weights(\"../model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f36c30-605a-4a0c-85a1-9a01b89d6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the history dictionary\n",
    "df_plot = pd.DataFrame(model.history, index=range(1, len(model.history[\"loss\"]) + 1))\n",
    "\n",
    "# Plot the loss\n",
    "df_plot.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257981c-0714-477a-9c65-9c56dbf20360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "df_plot.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b492307-600b-459f-b47a-88e22a2fe508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model fit with linear dummy data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ff352-1d66-4de4-acb8-b452326d89ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyvizlearn] *",
   "language": "python",
   "name": "conda-env-pyvizlearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
